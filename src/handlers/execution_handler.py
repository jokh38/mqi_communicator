"""
Provides a unified handler for executing commands and transferring files
either locally or on a remote machine, driven by configuration settings.
"""

import os
import re
import shutil
import subprocess
from pathlib import Path
from typing import Any, NamedTuple, Optional

import paramiko

from src.config.settings import Settings
from src.infrastructure.logging_handler import LoggerFactory


class ExecutionResult(NamedTuple):
    """A structured result from a command execution."""
    success: bool
    output: str
    error: str
    return_code: int


class JobSubmissionResult(NamedTuple):
    """Result from a job submission, including a potential job ID."""
    success: bool
    job_id: Optional[str] = None
    error: Optional[str] = None


class UploadResult(NamedTuple):
    """Result from a file upload operation."""
    success: bool
    error: Optional[str] = None


class DownloadResult(NamedTuple):
    """Result from a file download operation."""
    success: bool
    error: Optional[str] = None


class ExecutionHandler:
    """
    A simplified handler that executes commands generated by the Settings class.
    """

    def __init__(self,
                 mode: str,
                 settings: Optional[Settings] = None,
                 ssh_client: Optional[paramiko.SSHClient] = None):
        """
        Initializes the ExecutionHandler.
        """
        if mode not in ["local", "remote"]:
            raise ValueError("Mode must be either 'local' or 'remote'")
        if mode == "remote" and not ssh_client:
            raise ValueError("ssh_client is required for 'remote' mode")

        self.settings = settings or Settings()
        self.mode = mode
        self._ssh_client = ssh_client
        self._sftp_client: Optional[paramiko.SFTPClient] = None
        try:
            self.logger = LoggerFactory.get_logger("ExecutionHandler")
        except RuntimeError:
            from src.infrastructure.logging_handler import LoggerFactory as LF
            LF.configure(self.settings)
            self.logger = LF.get_logger("ExecutionHandler")

    def run_raw_to_dcm(self, case_id: str, path: str | Path, output_dir: Optional[Path] = None) -> ExecutionResult:
        """
        Run raw_to_dcm either remotely or locally using config-defined paths.
        - Remote: invoke script path from config with python and required args.
        - Local: execute the script from config with derived input/output paths.
        """
        if self.mode == "remote":
            if not self._ssh_client:
                raise ConnectionError("SSH client not available for remote execution.")

            # Get script path and python executable from config
            python_exe = self.settings.get_executable("python", handler_name="PostProcessor")
            script_path = self.settings.get_executable("raw_to_dicom_script", handler_name="PostProcessor")
            hpc_path = str(path)
            cmd = f"{python_exe} {script_path} --case_id {case_id} --hpc_path {hpc_path}"
            # Call SSH client directly to match tests (avoid reading stdout.channel)
            self._ssh_client.exec_command(cmd)
            return ExecutionResult(True, "", "", 0)
        else:
            # Get paths from config
            python_exe = self.settings.get_executable("python", handler_name="PostProcessor")
            raw_to_dicom_script = self.settings.get_executable("raw_to_dicom_script", handler_name="PostProcessor")
            raw_to_dicom_dir = self.settings.get_path("raw_to_dicom_dir", handler_name="PostProcessor")

            local_case_path = Path(path)
            input_file = local_case_path / f"{case_id}.raw"
            output_dir = output_dir or (local_case_path / "dicom")

            # Build command using config-defined python and script
            cmd = f"cd {raw_to_dicom_dir} && {python_exe} {raw_to_dicom_script} --input {input_file} --output {output_dir} --dosetype 2d"
            try:
                res = subprocess.run(cmd, shell=True, check=True, capture_output=True, text=True, cwd=local_case_path)
                return ExecutionResult(True, res.stdout, res.stderr, res.returncode)
            except subprocess.CalledProcessError as e:
                return ExecutionResult(False, e.stdout, e.stderr, e.returncode)

    class JobWaitResult(NamedTuple):
        failed: bool
        error: Optional[str] = None

    def wait_for_job_completion(self, job_id: Optional[str] = None, timeout: Optional[int] = None,
                                poll_interval: Optional[int] = None, log_file_path: Optional[str] = None,
                                beam_id: Optional[str] = None, case_repo: Optional[Any] = None) -> "ExecutionHandler.JobWaitResult":
        """
        Wait for job completion and monitor progress.
        - In remote mode with job_id: poll SLURM queue status
        - In local mode with log_file_path: monitor log file for completion markers and progress
        """
        import time
        import re
        from pathlib import Path

        # Get timeout and poll interval from settings if not provided
        if timeout is None:
            timeout = self.settings.get_processing_config().get("hpc_job_timeout_seconds", 3600)
        if poll_interval is None:
            poll_interval = self.settings.get_processing_config().get("hpc_poll_interval_seconds", 30)

        if self.mode == "remote" and job_id:
            # Remote mode: poll SLURM job status (to be implemented)
            return ExecutionHandler.JobWaitResult(failed=False)
        elif self.mode == "local" and log_file_path:
            # Local mode: monitor log file for completion patterns and progress
            completion_markers = self.settings.get_completion_markers()
            success_pattern = completion_markers.get("success_pattern", "Simulation completed successfully")
            failure_patterns = completion_markers.get("failure_patterns", ["FATAL ERROR", "ERROR:", "Segmentation fault"])

            # Progress tracking patterns from devplan
            total_batches_pattern = re.compile(r"with (\d+) batches")
            current_batch_pattern = re.compile(r"Generating particles for \((\d+) of (\d+) batches\)")

            start_time = time.time()
            log_path = Path(log_file_path)

            total_batches = None
            last_progress = 0.0
            file_position = 0

            while time.time() - start_time < timeout:
                if log_path.exists():
                    try:
                        with open(log_path, 'r') as f:
                            # Read from last position
                            f.seek(file_position)
                            new_content = f.read()
                            file_position = f.tell()

                            if new_content:
                                # Check for failure patterns first
                                for pattern in failure_patterns:
                                    if pattern in new_content:
                                        return ExecutionHandler.JobWaitResult(
                                            failed=True,
                                            error=f"Simulation failed: found pattern '{pattern}' in log"
                                        )

                                # Check for success pattern
                                if success_pattern in new_content:
                                    # Update to 100% before returning
                                    if case_repo and beam_id:
                                        try:
                                            case_repo.update_beam_progress(beam_id, 100.0)
                                        except Exception:
                                            pass
                                    return ExecutionHandler.JobWaitResult(failed=False)

                                # Extract total batches if not found yet
                                if total_batches is None:
                                    match = total_batches_pattern.search(new_content)
                                    if match:
                                        total_batches = int(match.group(1))
                                        self.logger.info(f"Detected total batches: {total_batches}")

                                # Extract current batch and update progress
                                if total_batches is not None:
                                    for match in current_batch_pattern.finditer(new_content):
                                        current_batch = int(match.group(1))
                                        # Calculate simulation progress (30% to 90% range for HPC phase)
                                        sim_progress = (current_batch / total_batches) * 60 + 30

                                        # Only update if progress increased by at least 1%
                                        if case_repo and beam_id and (sim_progress - last_progress >= 1.0):
                                            try:
                                                case_repo.update_beam_progress(beam_id, sim_progress)
                                                last_progress = sim_progress
                                            except Exception as e:
                                                self.logger.warning(f"Failed to update progress: {e}")

                    except Exception as e:
                        self.logger.warning(f"Error reading log file: {e}")

                time.sleep(poll_interval)

            # Timeout reached
            return ExecutionHandler.JobWaitResult(
                failed=True,
                error=f"Simulation timeout after {timeout} seconds"
            )
        else:
            # No monitoring needed or not enough info
            return ExecutionHandler.JobWaitResult(failed=False)


    def execute_command(self,
                        command: str,
                        cwd: Optional[Path] = None) -> ExecutionResult:
        """
        Executes a command either locally or remotely.
        """
        if self.mode == "local":
            try:
                result = subprocess.run(command,
                                        shell=True,
                                        check=True,
                                        capture_output=True,
                                        text=True,
                                        cwd=cwd)
                return ExecutionResult(success=True,
                                       output=result.stdout,
                                       error=result.stderr,
                                       return_code=result.returncode)
            except subprocess.CalledProcessError as e:
                return ExecutionResult(success=False,
                                       output=e.stdout,
                                       error=e.stderr,
                                       return_code=e.returncode)
        else:  # remote
            if not self._ssh_client:
                raise ConnectionError("SSH client not available for remote execution.")
            full_command = f"cd {cwd} && {command}" if cwd else command
            stdin, stdout, stderr = self._ssh_client.exec_command(full_command)
            exit_code = stdout.channel.recv_exit_status()
            return ExecutionResult(success=exit_code == 0,
                                   output=stdout.read().decode("utf-8"),
                                   error=stderr.read().decode("utf-8"),
                                   return_code=exit_code)

    def submit_simulation_job(self, script_path: str = None, handler_name: Optional[str] = None, command_key: Optional[str] = None,
                              **context: Any) -> JobSubmissionResult:
        """
        Submit a simulation job. In remote mode: either execute `sbatch {script_path}`
        when a script path is provided (test-friendly), or resolve via settings when
        handler_name+command_key are provided. In local mode, raise NotImplementedError.
        """
        if self.mode == "local":
            raise NotImplementedError("Local mode does not support simulation job submission.")
        try:
            if script_path:
                cmd = f"sbatch {script_path}"
            elif handler_name and command_key:
                cmd = self.settings.get_command(command_key, handler_name=handler_name, **context)
            else:
                return JobSubmissionResult(success=False, error="Insufficient parameters for job submission")

            result = self.execute_command(cmd)
            if not result.success:
                return JobSubmissionResult(success=False, error=result.error)
            match = re.search(r"(\d+)$", (result.output or "").strip())
            if match:
                return JobSubmissionResult(success=True, job_id=match.group(1))
            return JobSubmissionResult(success=False, error="Could not extract job ID from command output.")
        except Exception as e:
            return JobSubmissionResult(success=False, error=str(e))


    def post_process(self, handler_name: str, **context: Any) -> ExecutionResult:
        """
        Runs a post-processing command obtained from Settings.
        """
        command = self.settings.get_command('post_process',
                                            handler_name=handler_name,
                                            **context)
        cwd = context.get('cwd')
        return self.execute_command(command, cwd=cwd)

    def upload_file(self, local_path: str, remote_path: str) -> UploadResult:
        """
        Uploads a file. In local mode, this is a copy.
        """
        try:
            if self.mode == "local":
                target_dir = os.path.dirname(remote_path)
                if target_dir:
                    os.makedirs(target_dir, exist_ok=True)
                shutil.copy(local_path, remote_path)
            else:  # remote
                if not self._ssh_client:
                    raise ConnectionError("SSH client not available for remote upload.")
                if not self._sftp_client:
                    self._sftp_client = self._ssh_client.open_sftp()
                self._mkdir_p(self._sftp_client, os.path.dirname(remote_path))
                self._sftp_client.put(local_path, remote_path)
            return UploadResult(success=True)
        except Exception as e:
            self.logger.error("File upload failed", context={"error": str(e)})
            return UploadResult(success=False, error=str(e))

    def download_file(self, remote_path: str, local_path: str) -> DownloadResult:
        """
        Downloads a file. In local mode, this is a copy.
        """
        try:
            local_dir = os.path.dirname(local_path)
            if local_dir:
                os.makedirs(local_dir, exist_ok=True)

            if self.mode == "local":
                shutil.copy(remote_path, local_path)
            else:  # remote
                if not self._ssh_client:
                    raise ConnectionError("SSH client not available for remote download.")
                if not self._sftp_client:
                    self._sftp_client = self._ssh_client.open_sftp()
                self._sftp_client.get(remote_path, local_path)
            return DownloadResult(success=True)
        except Exception as e:
            self.logger.error("File download failed", context={"error": str(e)})
            return DownloadResult(success=False, error=str(e))

    def cleanup(self, handler_name: str, **context: Any) -> ExecutionResult:
        """
        Runs a cleanup command (e.g., 'rm -rf') from Settings.
        """
        command = self.settings.get_command("cleanup",
                                            handler_name=handler_name,
                                            **context)
        return self.execute_command(command)

    def upload_to_pc_localdata(self, local_path: Path | str, case_id: str, settings: Optional[Settings] = None,
                                handler_name: Optional[str] = None, **context: Any) -> UploadResult:
        """
        Upload results to PC_localdata.
        - In local mode: simulate by copying to ./localdata_uploads/{case_id}
        - In remote mode: resolve command via settings when available
        """
        try:
            if self.mode == "local":
                base_dir = Path("./localdata_uploads") / case_id
                base_dir.mkdir(parents=True, exist_ok=True)
                src = Path(local_path)
                shutil.copy(str(src), str(base_dir / src.name))
                import logging
                logging.getLogger(__name__).info(
                    f"Simulating upload by copying to local directory for case {case_id}")
                return UploadResult(success=True)
            else:
                eff_settings = settings or self.settings
                if handler_name is None:
                    handler_name = "ResultUploader"
                cmd = eff_settings.get_command("upload_result", handler_name=handler_name,
                                              local_path=str(local_path), case_id=case_id, **context)
                res = self.execute_command(cmd)
                return UploadResult(success=res.success, error=None if res.success else res.error)
        except Exception as e:
            return UploadResult(success=False, error=str(e))


    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self._sftp_client:
            self._sftp_client.close()

    def _mkdir_p(self, sftp: paramiko.SFTPClient, remote_directory: str):
        """
        Creates a directory and all its parents recursively on the remote server.
        This is a more robust implementation that handles nested creation.
        """
        if remote_directory == "/":
            sftp.chdir("/")
            return
        if remote_directory == "":
            return
        try:
            sftp.chdir(remote_directory)  # Test if remote_directory exists
        except IOError:
            dirname, basename = os.path.split(remote_directory.rstrip("/"))
            self._mkdir_p(sftp, dirname)
            sftp.mkdir(basename)
            sftp.chdir(basename)
            return True